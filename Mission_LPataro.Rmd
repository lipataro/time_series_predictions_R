---
title: "Mission - Data Exploration and Statistics in R"
subtitle: "TIME SERIES ANALYSIS"
author: "Lisandra Pataro"
date: "1/28/2022"
output:
  html_document: default
---

### Reading and Preparing the data

First, we read the data from a .csv file and adjust the columns' units using ***mutate***. The first activity I've chosen to make the time analysis was [virtual ride]{.ul}, but there is not enough historical data to predict one year ahead. The activity starts only in 2020, what can compromise the future predictions.

```{r}

require(tidyverse)
require(lubridate)

dat <- read.csv("Activities_2022.csv",header = T)

dat <- dat %>% 
  mutate(start_time=as_datetime(startTimeLocal/1000), # create a timestamp
         date = floor_date(start_time,"month"), # round to the day
         distance=distance/1E5, # convert in km
         calories = calories/4.184) %>% # convert from joules in calories
  mutate(across(contains("elevation"),function(xx) xx/1E2))  %>%  # convert to meters
  mutate(across(contains("Speed"),function(xx) xx*36))  %>% # convert to km/h
  mutate(across(c(duration,contains("Duration")),
                function(xx) time_length(xx/1000,"minutes"))) %>% 
  
  mutate(is_virtualride=ifelse(activityType == "virtual_ride",T,F))
         # selecting the Activity type 
dat_clean <- filter(dat,!(activityId %in% c(407226313,2321338)) & year(date)>=2012) %>% 
  filter(avgSpeed<60 & avgSpeed>1) %>% 
  filter(distance>0) %>% 
  filter(duration>0)
```

So, lets try [cycling]{.ul} and check the data.

```{r Read_Activity, echo=TRUE}
dat_clean <- mutate(dat_clean,is_cycling=ifelse(activityType == "cycling",T,F))
dat_cycling <- filter(dat_clean,is_cycling) %>% # Only cycling 
  select(date,avgSpeed,distance,duration,avgHr,avgPower,elevationGain,
         max20MinPower,calories,normPower,maxBikeCadence,avgBikeCadence)
dat_cycling <- dat_cycling %>%
  select(date, duration, avgSpeed, distance, avgHr, elevationGain)
head(dat_cycling)
```

It looks like we have consistent data since 2012. It is important to say that we want monthly predictions for our series. So it is necessary to aggregate the daily observations into months. One way of doing that is using the ***apply.monthly*** function from the **xts** library.

Let's see that working with the average speed: **avgSpeed**. First we remove the rows with NAs and then we complete the series with the months without metrics. The command ***complete*** completes the missing months with **NAs**.

```{r echo=TRUE, warning=FALSE}
dat_cycling_avgSpeed <- dat_cycling %>%
  select(date,avgSpeed) %>%
  na.omit() %>% 
  complete(date = seq.Date(min(as.Date(date)), max(as.Date(date)), by="month"))
library(xts)
xts2 <- xts(dat_cycling_avgSpeed$avgSpeed, dat_cycling_avgSpeed$date)
series <- apply.monthly(xts2,mean)
head(series)
#plot(dat_cycling$date,dat_cycling$avgSpeed,ylab='avgSpeed', xlab='time')
ggplot(dat_cycling_avgSpeed, aes(date,avgSpeed)) + geom_point(size=1)
```

The frequency distribution for our data is skewed. The cycling activities do not seem to have the same goal. There are very long cycling activities and with very high average speeds. After 2014, there is more consistency with the two types of activities we have in this graph. Maybe it would be a good idea to separate those activities in "High/Lower avgSpeed Cycling". But let's start our analysis with the mean value of the entire data above.

Before we start the time series analysis, we have the choice of imputing the missing values. The Kalman filters are a good option on imputing values in time series. So, we use the library ***imputeTS*** and the function ***na_kalman*** for this task. The plots below show the missing values and new values that were inserted in the series.

```{r Impute}
library(imputeTS)
series_imp <- na_kalman(series)
head(series_imp)
```

```{r Plot_imputation, echo=TRUE, fig.height=3, fig.width=4}
ggplot_na_distribution(series)
ggplot_na_imputations(series, series_imp)
```

**Stationarity** means that the time series does not have a trend, has a constant variance, a constant autocorrelation pattern, and no seasonal pattern.

From the plot above, we can already see that the variance is not constant, as well as the mean value. therefore, it looks like a non-stationary process.

### AC and PAC Functions:

From the Autocorrelation plot, we can observe a strong correlation at the first couple of lags. The function drops slowly, which is a characteristic of **non-stationary** processes.

We can see the presence of **seasonal patterns** (although not very clear), because the auto-correlations are larger for lags at multiples of the seasonal frequency than for other lags.

```{r echo=TRUE, fig.height=7, fig.width=4}
library(forecast)
par(mfrow=c(3,1))
Acf(series_imp)
Pacf(series_imp)
Acf(diff(series_imp))

```

### Stationarity Tests

We chose two stationarity tests to be performed.

**Ljung-Box Test:**

Here the null hypothesis is that there is no autocorrelation between the different lags. A significant p-value (\<0.05) in this test **rejects the** null hypothesis that the time series isn't autocorrelated. Therefore, in the case of our series, the test shows **non-stationarity.**

```{r echo=TRUE}
Box.test(series_imp, lag=10, type="Ljung-Box")
```

**KPSS Unit Root Test:**

We use the *Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test*, where the null hypothesis is that the data are stationary. Small p-values (e.g., less than 0.1) suggest that the series is not stationary and differencing is required. And, in fact, our series is, according to the test, not stationary. However, the differencing (diff) of our series shows a KPSS test with a p-value>0.1, what we would expect for stationary data.

```{r echo=TRUE}
library(tseries)
kpss.test(series_imp)
kpss.test(diff(series_imp))
```

### Predictions using AUTO.ARIMA:

Returns best ***ARIMA*** model according to either AIC, AICc or BIC value.

For ARIMA to perform at its best it needs the data to be stationary. That means that the mean and variance are constant over the entire set. Differencing is used to transform the data so that it is stationary. So, we expect d =1 in Arima(p,d,q). To minimize variations in the variance, the transformation of BoxCox can be used.

We expect also a SARIMA (with seasonal parameters p,d,q) because of the presence of not very clear **seasonal patterns**.

```{r}
tseries<- ts(series_imp, frequency=12, start=c(2012,1))
Lambda <- BoxCox.lambda(tseries); Lambda
prev <- auto.arima(tseries, seasonal=T,lambda=Lambda) #removed the transf (lambda=Lambda)
fcast <- forecast(prev, h=12, level=c(90)); fcast
plot(fcast, col=1, main="Activity:Cycling", type="l", lwd="2", ylab="avgSpeed",xlab="Time")
```

The residuals in ARIMA models tell a story about the performance of our model and should be taken into consideration when evaluating them. Here we see residuals that are not much correlated and have reasonably good behavior regarding the zero-mean and Gaussian-like distribution.

```{r}
checkresiduals(prev)
```

### Predictions using PROPHET:

Prophet is an open source library published by Facebook that is based on decomposable (trend+seasonality+holidays) models. It provides us with the ability to make time series predictions with good accuracy using simple intuitive parameters and has support for including impact of custom seasonality and holidays.

There is clearly seasonality in the prophet forecast, predicting a period of lower cycling average speeds during the winter and an increasing speed towards the European summer.

```{r }
library(prophet)
tseries2 <- fortify(series_imp) %>% 
  mutate(ds = as.Date(Index),y = series_imp)

prev2 <- prophet(tseries2)
future <- make_future_dataframe(prev2, periods=12, freq = "month")

forecast <- predict(prev2, future)
prophet_plot_components(prev2, forecast)
plot(prev2, forecast, main='avgSpeed forecast for Cycling activity',xlab="Time",ylab="avgSpeed (km/h)")
```

### Predictions for the Average Heart rate *AvgHr*  in *Cycling*:

We are going to run everything again, for the ***AvgHr*** and changing the monthly aggregation to **median**, which seems more reasonable.

```{r echo=TRUE, warning=FALSE}
dat_cycling_avgHr <- dat_cycling %>%
  select(date,avgHr) %>%
  na.omit() %>% 
  complete(date = seq.Date(min(as.Date(date)), max(as.Date(date)), by="month"))

xts2 <- xts(dat_cycling_avgHr$avgHr, dat_cycling_avgHr$date)
series <- apply.monthly(xts2,median)
ggplot(dat_cycling_avgHr, aes(date,avgHr)) + geom_point(size=1)
```

The plot of Average Heart rate shows a change on type of activity also. From 2017 to 2019-2020 we see more frequent cycling trainings with lower HRs. This will, for sure, have an impact in our predictions.

```{r fig.height=5, fig.width=4}
series_imp <- na_kalman(series)
head(series_imp)
ggplot_na_imputations(series, series_imp)
par(mfrow=c(3,1))
Acf(series_imp)
Pacf(series_imp)
Acf(diff(series_imp))
Box.test(series_imp, lag=10, type="Ljung-Box")
kpss.test(series_imp)
```

Here we have doubts about the stationarity of the series (avgHr). Maybe a BoxCox transformation can reduce the discrepancy in variance.

```{r ARIMA}
tseries<- ts(series_imp, frequency=12, start=c(2012,1))
Lambda <- BoxCox.lambda(tseries); 
prev <- auto.arima(tseries, lambda=Lambda) 
fcast <- forecast(prev, h=12, level=c(90)); fcast
par(mfrow=c(1,1))
plot(fcast, col=1, main="Activity:Cycling", type="l", lwd="2", ylab= "avgHr", xlab="Time")
checkresiduals(prev)
```

The residuals are not correlated and have reasonably good behavior regarding the zero-mean and Gaussian-like distribution, with good Ljung-Box test for stationarity.

The prophet below also predicts a period (winter) with less intense (lower average heart rate) cycling, which changes in the beginning of the year, getting more intense towards the European summer.

```{r Prophet, echo=TRUE, warning=FALSE}
tseries2 <- fortify(series_imp) %>% 
  mutate(ds = as.Date(Index),y = series_imp)

prev2 <- prophet(tseries2)
future <- make_future_dataframe(prev2, periods=12, freq="month") 

forecast <- predict(prev2, future)
plot(prev2, forecast, main='avgHr forecast for Cycling activity',xlab="Time",ylab="avgHr")
prophet_plot_components(prev2, forecast)
```

For the continuity of the work, I would like to make a function that executes the whole process for a certain metric.
